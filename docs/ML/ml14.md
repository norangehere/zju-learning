# 特征选择和稀疏学习

## 子集搜索与子集评价

1. 子集搜索
      - 前向搜索：逐渐增加相关特征
      - 后向搜索：从完整的特征集合开始，逐渐减少特征
      - 双向搜索：每一轮逐渐增加相关特征，同时减少无关特征

2. 评价特征子集对数据集划分和样本标记划分的差异，差异越小则说明当前特征子集越好
      - 信息增益

## 特征选择

1. 过滤式选择：先用特征选择过程过滤原始数据，再用过滤后的特征来训练模型；特征选择过程与后续学习器无关

      - Relief方法

        <div align="center"><img src="https://pixe1ran9e.oss-cn-hangzhou.aliyuncs.com/image-20250104195157128.png" alt="image-20250104195157128" style="zoom:67%;" /></div>

      - 相关统计量越大，属性j上，猜对近邻比猜错近邻越近，即属性j对区分对错越有用

2. 包裹式选择：直接把最终将要使用的学习器的性能作为特征子集的评价准则

      - LVW（Las Vegas Wrapper）
        - 在循环的每一轮随机产生一个特征子集
        - 在随机产生的特征子集上通过交叉验证推断当前特征子集的误差
        - 进行多次循环，在多个随机产生的特征子集中选择误差最小的特征子集作为最终解
          - 若有运行时间限制，则该算法有可能给不出解

3. 嵌入式选择：将特征选择过程与学习器训练过程融为一体，两者在同一个优化过程中完成，在学习器训练过程中自动地进行特征选择

## 稀疏表示

1. 字典学习：为普通稠密表达的样本找到合适的**字典**，将样本转化为稀疏表示

   <div align="center"><img src="https://pixe1ran9e.oss-cn-hangzhou.aliyuncs.com/image-20250104195642517.png" alt="image-20250104195642517" style="zoom:60%;" /></div>

## 压缩感知

1. 限定等距性：若存在常数$\delta_k\in(0,1)$使得对于任意向量$s$和$A$的所有子矩阵$A_k\in\mathbb{R}^{n\times k}$有：
   $$
   (1-\delta_k)\|s\|^2_2\leq\|A_k s\|^2_2\leq(1+\delta_k)\|s\|^2_2
   $$
   则称$A$满足$k$限定等距性 (k-RIP)，此时可以通过下面的优化问题近乎完美地从y恢复出稀疏信号s，进而恢复出x。若下式使用$L_0$范数则是NP hard
   $$
   \min_s\|s\|_1\newline   
   s.t. \ \ y=As
   $$

2. 矩阵补全
   
   $$
   \begin{aligned}
    &\min_X rank(X)\newline     
    s.t. \quad &(X)_{ij}=(A)_{ij},(i,j)\in\Omega
    \end{aligned}
   $$

